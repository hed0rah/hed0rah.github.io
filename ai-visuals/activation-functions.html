<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Activation Functions</title>
<style>
@import url('https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&family=Bebas+Neue&display=swap');
:root {
  --bg:        #0a0a0a;
  --surface:   #111111;
  --surface2:  #0d0d0d;
  --border:    #1e1e1e;
  --border2:   #2a2a2a;
  --cyan:      #00ffe9;
  --yellow:    #ffe900;
  --magenta:   #e900ff;
  --red:       #ff2d55;
  --green:     #39ff14;
  --orange:    #ff6b00;
  --dim:       #3a3a3a;
  --text:      #cccccc;
  --bright:    #ffffff;
}
*{margin:0;padding:0;box-sizing:border-box;}
body{background:var(--bg);color:var(--text);font-family:'Space Mono',monospace;font-size:11px;line-height:1.65;padding:36px 30px;max-width:980px;margin:0 auto;}
body::before{content:'';position:fixed;inset:0;background:repeating-linear-gradient(0deg,transparent,transparent 2px,rgba(0,0,0,0.04) 2px,rgba(0,0,0,0.04) 4px);pointer-events:none;z-index:999;}
.masthead{border-top:3px solid var(--cyan);padding:14px 0 10px;margin-bottom:28px;display:flex;justify-content:space-between;align-items:flex-end;border-bottom:1px solid var(--border);}
.masthead h1{font-family:'Bebas Neue',sans-serif;font-size:64px;letter-spacing:4px;color:var(--bright);line-height:1;}
.masthead h1 em{color:var(--cyan);font-style:normal;}
.masthead-right{text-align:right;font-size:10px;color:#444;letter-spacing:2px;text-transform:uppercase;}
.masthead-right strong{display:block;color:var(--cyan);font-size:11px;margin-bottom:2px;}
.g2{display:grid;grid-template-columns:1fr 1fr;gap:2px;margin-bottom:2px;}
.g3{display:grid;grid-template-columns:1fr 1fr 1fr;gap:2px;margin-bottom:2px;}
.gfull{margin-bottom:2px;}
.panel{background:var(--surface);border:1px solid var(--border);padding:18px;}
.plabel{font-family:'Bebas Neue',sans-serif;font-size:10px;letter-spacing:3px;color:#383838;text-transform:uppercase;margin-bottom:10px;padding-bottom:6px;border-bottom:1px solid var(--border);}
.ptitle{font-family:'Bebas Neue',sans-serif;font-size:26px;letter-spacing:2px;color:var(--bright);margin-bottom:8px;line-height:1;}
.ptitle.cy{color:var(--cyan);}.ptitle.rd{color:var(--red);}.ptitle.yl{color:var(--yellow);}.ptitle.pu{color:var(--magenta);}.ptitle.or{color:var(--orange);}.ptitle.gr{color:var(--green);}
.bar{width:28px;height:2px;margin-bottom:10px;background:var(--cyan);}
.bar.rd{background:var(--red);}.bar.yl{background:var(--yellow);}.bar.pu{background:var(--magenta);}.bar.or{background:var(--orange);}
p{font-size:11px;color:var(--text);margin-bottom:7px;line-height:1.65;}
.hl{color:var(--cyan);font-weight:700;}.hl.rd{color:var(--red);}.hl.yl{color:var(--yellow);}.hl.pu{color:var(--magenta);}.hl.or{color:var(--orange);}.hl.gr{color:var(--green);}
.math-block{background:#060606;border:1px solid var(--border);border-left:3px solid var(--yellow);padding:12px 14px;margin:10px 0;font-size:11px;}
.math-block.cy{border-left-color:var(--cyan);}.math-block.rd{border-left-color:var(--red);}
.eq{color:var(--yellow);}.eq.cy{color:var(--cyan);}.eq.rd{color:var(--red);}
.org-row{display:flex;justify-content:space-between;padding:5px 0;border-bottom:1px solid #111;font-size:10px;}
.org-row:last-child{border-bottom:none;}
.org-row-label{color:#333;text-transform:uppercase;letter-spacing:1px;font-size:9px;}
.org-row-val{font-weight:700;}
.tag{display:inline-block;border:1px solid var(--border);padding:1px 6px;font-size:9px;letter-spacing:1px;text-transform:uppercase;color:#444;margin:2px 2px 2px 0;}
.tag.cy{border-color:var(--cyan);color:var(--cyan);}.tag.rd{border-color:var(--red);color:var(--red);}.tag.yl{border-color:var(--yellow);color:var(--yellow);}.tag.gr{border-color:var(--green);color:var(--green);}
.footer{border-top:1px solid var(--border);margin-top:14px;padding-top:10px;display:flex;justify-content:space-between;font-size:9px;color:#2a2a2a;letter-spacing:1px;}
canvas{display:block;background:#060606;}
button{font-family:'Space Mono',monospace;font-size:10px;letter-spacing:2px;text-transform:uppercase;padding:8px 14px;border:1px solid var(--border2);background:var(--surface2);color:var(--dim);cursor:pointer;transition:all 0.1s;}
button:hover{background:var(--border2);color:var(--bright);}
button.act{background:var(--cyan);color:var(--bg);border-color:var(--cyan);}
input[type=range]{-webkit-appearance:none;width:100%;height:2px;background:var(--border2);border-radius:1px;outline:none;margin:6px 0;}
input[type=range]::-webkit-slider-thumb{-webkit-appearance:none;width:12px;height:12px;background:var(--bright);border-radius:50%;cursor:pointer;}
@media(max-width:700px){.g2,.g3{grid-template-columns:1fr;}}
</style>
</head>
<body>

<div class="masthead">
  <h1>ACTIV<em>ATION</em><br>FUNCTIONS</h1>
  <div class="masthead-right">
    <strong>THE NONLINEARITY THAT MAKES NETWORKS WORK</strong>
    SIGMOID // TANH // RELU // GELU<br>
    VANISHING GRADIENTS // DYING NEURONS
  </div>
</div>

<div class="gfull">
  <div class="panel">
    <div class="plabel">01 // What They Are</div>
    <div class="ptitle cy">WHY NONLINEARITY EXISTS</div>
    <div class="bar"></div>
    <div style="display:grid;grid-template-columns:2fr 1fr;gap:20px;">
      <div>
        <p>Without an activation function, stacking linear layers is mathematically equivalent to a single linear transformation. No matter how deep the network, it can only learn linear mappings — useless for anything interesting. Activation functions introduce nonlinearity at each neuron, allowing the network to approximate arbitrarily complex functions.</p>
        <p>The activation function is applied element-wise to the pre-activation value <span class="hl">z = Wx + b</span>. Its output becomes the neuron's activation — what flows to the next layer. The choice of function determines gradient behavior across layers, which in turn determines whether training works at all in deep architectures.</p>
        <p>The history of deep learning is partly the history of finding better activations. Sigmoid dominated until ~2010. ReLU unlocked deep networks. GELU now dominates transformers. The ideal function is nonlinear, computationally cheap, and keeps gradients alive through many layers.</p>
      </div>
      <div>
        <div style="display:grid;grid-template-columns:1fr 1fr;gap:8px;text-align:center;">
          <div><div style="font-family:'Bebas Neue',sans-serif;font-size:32px;color:var(--cyan);line-height:1;">1943</div><div style="font-size:9px;color:#444;letter-spacing:1px;text-transform:uppercase;">McCulloch-Pitts<br>step function</div></div>
          <div><div style="font-family:'Bebas Neue',sans-serif;font-size:32px;color:var(--yellow);line-height:1;">1986</div><div style="font-size:9px;color:#444;letter-spacing:1px;text-transform:uppercase;">Sigmoid + backprop<br>paper</div></div>
          <div><div style="font-family:'Bebas Neue',sans-serif;font-size:32px;color:var(--red);line-height:1;">2010</div><div style="font-size:9px;color:#444;letter-spacing:1px;text-transform:uppercase;">ReLU proven<br>in deep nets</div></div>
          <div><div style="font-family:'Bebas Neue',sans-serif;font-size:32px;color:var(--magenta);line-height:1;">2016</div><div style="font-size:9px;color:#444;letter-spacing:1px;text-transform:uppercase;">GELU introduced<br>for transformers</div></div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="g2">
  <div class="panel">
    <div class="plabel">02 // Sigmoid</div>
    <div class="ptitle cy">SIGMOID</div>
    <div class="bar"></div>
    <div class="math-block cy"><span class="eq cy">f(x) = 1 / (1 + e^(-x))</span><br><span style="color:#444;font-size:10px;">derivative: f'(x) = f(x)(1 - f(x)) &nbsp; max: 0.25</span></div>
    <canvas id="cvSig" width="400" height="160" style="width:100%;height:160px;margin-bottom:10px;"></canvas>
    <div class="org-row"><span class="org-row-label">Output range</span><span class="org-row-val">(0, 1)</span></div>
    <div class="org-row"><span class="org-row-label">Max gradient</span><span class="org-row-val" style="color:var(--red);">0.25 at x=0</span></div>
    <div class="org-row"><span class="org-row-label">Differentiable</span><span class="org-row-val" style="color:var(--green);">everywhere</span></div>
    <div class="org-row"><span class="org-row-label">Zero-centered</span><span class="org-row-val" style="color:var(--red);">no</span></div>
    <div class="org-row"><span class="org-row-label">Era</span><span class="org-row-val">1980s - ~2012</span></div>
    <p style="margin-top:10px;font-size:10px;color:#555;">Classic S-curve. Saturates hard at both extremes — gradient approaches zero outside [-4, 4]. In a 10-layer network, sigmoid gradient attenuates to 0.25^10 ≈ 9.5×10^-7 at the first layer. Training becomes impossible. This is the vanishing gradient problem.</p>
    <div style="margin-top:6px;"><span class="tag cy">smooth</span><span class="tag rd">vanishing grad</span><span class="tag">output layer only</span></div>
  </div>

  <div class="panel">
    <div class="plabel">03 // ReLU</div>
    <div class="ptitle rd">RELU</div>
    <div class="bar rd"></div>
    <div class="math-block rd"><span class="eq rd">f(x) = max(0, x)</span><br><span style="color:#444;font-size:10px;">derivative: f'(x) = 0 if x &lt; 0, else 1</span></div>
    <canvas id="cvRelu" width="400" height="160" style="width:100%;height:160px;margin-bottom:10px;"></canvas>
    <div class="org-row"><span class="org-row-label">Output range</span><span class="org-row-val">[0, +inf)</span></div>
    <div class="org-row"><span class="org-row-label">Max gradient</span><span class="org-row-val" style="color:var(--green);">1 (no decay)</span></div>
    <div class="org-row"><span class="org-row-label">Differentiable</span><span class="org-row-val" style="color:var(--red);">not at x=0</span></div>
    <div class="org-row"><span class="org-row-label">Zero-centered</span><span class="org-row-val" style="color:var(--red);">no</span></div>
    <div class="org-row"><span class="org-row-label">Era</span><span class="org-row-val">2010s - present</span></div>
    <p style="margin-top:10px;font-size:10px;color:#555;">Rectified Linear Unit. AlexNet (2012) used it to win ImageNet by a large margin — the paper that reset the field. Gradient is 1 on the positive side so it passes through layers unchanged. Creates sparse activations: ~50% of neurons output zero at any time, which turns out to be efficient and regularizing.</p>
    <div style="margin-top:6px;"><span class="tag rd">dying neurons</span><span class="tag gr">fast</span><span class="tag gr">no vanishing</span></div>
  </div>
</div>

<div class="g2">
  <div class="panel">
    <div class="plabel">04 // Tanh</div>
    <div class="ptitle yl">TANH</div>
    <div class="bar yl"></div>
    <div class="math-block"><span class="eq">f(x) = (e^x - e^-x) / (e^x + e^-x)</span><br><span style="color:#444;font-size:10px;">derivative: f'(x) = 1 - tanh²(x) &nbsp; max: 1.0</span></div>
    <canvas id="cvTanh" width="400" height="140" style="width:100%;height:140px;margin-bottom:10px;"></canvas>
    <div class="org-row"><span class="org-row-label">Output range</span><span class="org-row-val">(-1, 1)</span></div>
    <div class="org-row"><span class="org-row-label">Max gradient</span><span class="org-row-val" style="color:var(--yellow);">1.0 at x=0</span></div>
    <div class="org-row"><span class="org-row-label">Zero-centered</span><span class="org-row-val" style="color:var(--green);">yes</span></div>
    <p style="margin-top:10px;font-size:10px;color:#555;">Rescaled sigmoid centered at zero. Being zero-centered helps gradient updates point in consistent directions — sigmoid's positive-only output causes zig-zagging optimization. Still saturates, still vanishes in deep nets, but better than sigmoid for hidden layers. Common in RNNs.</p>
    <div style="margin-top:6px;"><span class="tag yl">zero-centered</span><span class="tag rd">saturates</span><span class="tag">RNN staple</span></div>
  </div>

  <div class="panel">
    <div class="plabel">05 // GELU</div>
    <div class="ptitle pu">GELU</div>
    <div class="bar pu"></div>
    <div class="math-block" style="border-left-color:var(--magenta);"><span class="eq" style="color:var(--magenta);">f(x) = x · Φ(x)</span><br><span style="color:#444;font-size:10px;">where Φ(x) is the Gaussian CDF — smooth stochastic gating</span></div>
    <canvas id="cvGelu" width="400" height="140" style="width:100%;height:140px;margin-bottom:10px;"></canvas>
    <div class="org-row"><span class="org-row-label">Output range</span><span class="org-row-val">(-0.17, +inf)</span></div>
    <div class="org-row"><span class="org-row-label">Zero crossing</span><span class="org-row-val" style="color:var(--magenta);">smooth, not hard</span></div>
    <div class="org-row"><span class="org-row-label">Used in</span><span class="org-row-val">BERT, GPT, all modern</span></div>
    <p style="margin-top:10px;font-size:10px;color:#555;">Gaussian Error Linear Unit (Hendrycks &amp; Gimpel, 2016). Instead of hard gating negative values to zero like ReLU, GELU weights inputs by their probability under a Gaussian — a soft stochastic gate. The small negative dip around x=-0.1 provides richer gradient signal. GPT-2 onward uses it exclusively.</p>
    <div style="margin-top:6px;"><span class="tag pu">transformers</span><span class="tag pu">BERT/GPT</span><span class="tag">smooth gate</span></div>
  </div>
</div>

<div class="gfull">
  <div class="panel">
    <div class="plabel">06 // Interactive // Vanishing Gradient Demo</div>
    <div class="ptitle rd">GRADIENT DECAY THROUGH LAYERS</div>
    <div class="bar rd"></div>
    <p>During backpropagation, each layer multiplies the incoming gradient by the local derivative of its activation function. With sigmoid, the max derivative is 0.25 — meaning at best 75% of gradient signal is destroyed at every layer. Stack 10 layers and the gradient reaching layer 1 is a ghost.</p>
    <div style="display:grid;grid-template-columns:1fr 200px;gap:20px;margin-top:14px;">
      <div id="gradBars"></div>
      <div>
        <div style="font-size:10px;color:#555;margin-bottom:10px;">Select activation:</div>
        <div style="display:flex;flex-direction:column;gap:4px;" id="actBtns">
          <button class="act" data-fn="sigmoid">Sigmoid</button>
          <button data-fn="relu">ReLU</button>
          <button data-fn="tanh">Tanh</button>
          <button data-fn="leaky">Leaky ReLU</button>
        </div>
        <div style="margin-top:16px;font-size:10px;color:#333;">Layers: <span id="layerCount" style="color:var(--bright);">10</span></div>
        <input type="range" id="layerSlider" min="3" max="20" value="10">
        <div style="margin-top:12px;padding:10px;background:#060606;border:1px solid var(--border);">
          <div style="font-size:9px;color:#333;letter-spacing:1px;text-transform:uppercase;margin-bottom:4px;">Gradient at layer 1</div>
          <div id="finalGrad" style="font-family:'Bebas Neue',sans-serif;font-size:28px;color:var(--red);">—</div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="gfull">
  <div class="panel">
    <div class="plabel">07 // Interactive // Live Output</div>
    <div class="ptitle cy">INPUT X RESPONSE</div>
    <div class="bar"></div>
    <p>Drag the slider to probe each function at a given input. Watch how sigmoid and tanh saturate in the extremes, flattening their gradients to near-zero. ReLU and GELU stay alive.</p>
    <div style="margin-top:10px;">
      <div style="display:flex;align-items:center;gap:12px;margin-bottom:14px;">
        <span style="font-size:10px;color:#555;width:50px;">x =</span>
        <input type="range" id="xSlider" min="-6" max="6" step="0.05" value="0" style="flex:1;">
        <span id="xVal" style="font-family:'Bebas Neue',sans-serif;font-size:24px;color:var(--bright);width:60px;">0.00</span>
      </div>
      <div style="display:grid;grid-template-columns:repeat(4,1fr);gap:2px;">
        <div style="background:#060606;border:1px solid var(--border);padding:12px;text-align:center;">
          <div style="font-size:9px;color:#333;letter-spacing:1px;text-transform:uppercase;margin-bottom:4px;">Sigmoid</div>
          <div id="outSig" style="font-family:'Bebas Neue',sans-serif;font-size:32px;color:var(--cyan);">0.5000</div>
          <div style="font-size:9px;color:#333;margin-top:2px;">grad: <span id="gradSig" style="color:var(--yellow);">0.2500</span></div>
        </div>
        <div style="background:#060606;border:1px solid var(--border);padding:12px;text-align:center;">
          <div style="font-size:9px;color:#333;letter-spacing:1px;text-transform:uppercase;margin-bottom:4px;">ReLU</div>
          <div id="outRelu" style="font-family:'Bebas Neue',sans-serif;font-size:32px;color:var(--red);">0.0000</div>
          <div style="font-size:9px;color:#333;margin-top:2px;">grad: <span id="gradRelu" style="color:var(--yellow);">0.0000</span></div>
        </div>
        <div style="background:#060606;border:1px solid var(--border);padding:12px;text-align:center;">
          <div style="font-size:9px;color:#333;letter-spacing:1px;text-transform:uppercase;margin-bottom:4px;">Tanh</div>
          <div id="outTanh" style="font-family:'Bebas Neue',sans-serif;font-size:32px;color:var(--yellow);">0.0000</div>
          <div style="font-size:9px;color:#333;margin-top:2px;">grad: <span id="gradTanh" style="color:var(--yellow);">1.0000</span></div>
        </div>
        <div style="background:#060606;border:1px solid var(--border);padding:12px;text-align:center;">
          <div style="font-size:9px;color:#333;letter-spacing:1px;text-transform:uppercase;margin-bottom:4px;">GELU</div>
          <div id="outGelu" style="font-family:'Bebas Neue',sans-serif;font-size:32px;color:var(--magenta);">0.0000</div>
          <div style="font-size:9px;color:#333;margin-top:2px;">grad: <span id="gradGelu" style="color:var(--yellow);">0.5000</span></div>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="g3">
  <div class="panel">
    <div class="plabel">08 // Variants</div>
    <div class="ptitle">LEAKY RELU</div>
    <div class="bar or"></div>
    <div class="math-block" style="border-left-color:var(--orange);"><span class="eq" style="color:var(--orange);">f(x) = x if x &gt; 0, else 0.01x</span></div>
    <p style="font-size:10px;color:#555;">Fixes dying ReLU by allowing a small negative slope (0.01). Neurons can never fully die — they still receive small gradients. Parametric ReLU (PReLU) learns this slope. Neither dominates ReLU in practice unless dead neuron rate is a measured problem.</p>
    <div style="margin-top:6px;"><span class="tag or">no dead neurons</span></div>
  </div>
  <div class="panel">
    <div class="plabel">09 // Variants</div>
    <div class="ptitle">SWISH / SILU</div>
    <div class="bar yl"></div>
    <div class="math-block"><span class="eq">f(x) = x · sigmoid(x)</span></div>
    <p style="font-size:10px;color:#555;">Self-gated activation discovered by neural architecture search (Ramachandran et al., 2017). Smooth, non-monotonic, unbounded above and bounded below. Outperforms ReLU on deep networks. Used in EfficientNet. SiLU is the same function — the name used in PyTorch.</p>
    <div style="margin-top:6px;"><span class="tag yl">NAS-discovered</span></div>
  </div>
  <div class="panel">
    <div class="plabel">10 // Variants</div>
    <div class="ptitle">ELU / SELU</div>
    <div class="bar pu"></div>
    <div class="math-block" style="border-left-color:var(--magenta);"><span class="eq" style="color:var(--magenta);">f(x) = x if x&gt;0, else α(e^x - 1)</span></div>
    <p style="font-size:10px;color:#555;">Exponential Linear Unit. Negative saturation is smooth rather than hard zero — mean activations can be pushed toward zero. SELU (Scaled ELU) adds a scale factor that induces self-normalizing behavior: activations remain near unit variance without batch normalization.</p>
    <div style="margin-top:6px;"><span class="tag pu">self-normalizing</span></div>
  </div>
</div>

<div class="footer">
  <span>RUMELHART ET AL. 1986 // NAIR &amp; HINTON 2010 // HENDRYCKS &amp; GIMPEL 2016</span>
  <span>BRUTALIST TERMINAL v2 // ACTIVATION FUNCTIONS</span>
</div>

<script>
function sigmoid(x){ return 1/(1+Math.exp(-x)); }
function sigmoidD(x){ const s=sigmoid(x); return s*(1-s); }
function relu(x){ return Math.max(0,x); }
function reluD(x){ return x>0?1:0; }
function tanhF(x){ return Math.tanh(x); }
function tanhD(x){ return 1-Math.tanh(x)**2; }
function gelu(x){
  return 0.5*x*(1+Math.tanh(Math.sqrt(2/Math.PI)*(x+0.044715*x*x*x)));
}
function geluD(x){
  const c=Math.sqrt(2/Math.PI);
  const t=Math.tanh(c*(x+0.044715*x*x*x));
  return 0.5*(1+t)+0.5*x*(1-t*t)*c*(1+3*0.044715*x*x);
}

function drawFn(id, fns, yMin, yMax){
  const cv=document.getElementById(id);
  const dpr=window.devicePixelRatio||1;
  const rect=cv.getBoundingClientRect();
  cv.width=rect.width*dpr; cv.height=rect.height*dpr;
  const ctx=cv.getContext('2d'); ctx.scale(dpr,dpr);
  const W=rect.width, H=rect.height;
  const pad=24;
  ctx.fillStyle='#060606'; ctx.fillRect(0,0,W,H);
  const xMin=-5, xMax=5;
  const toSx=x=>pad+(x-xMin)/(xMax-xMin)*(W-2*pad);
  const toSy=y=>H-pad-(y-yMin)/(yMax-yMin)*(H-2*pad);
  // grid
  ctx.strokeStyle='#1a1a1a'; ctx.lineWidth=1;
  for(let gx=-4;gx<=4;gx+=2){
    ctx.beginPath();ctx.moveTo(toSx(gx),pad);ctx.lineTo(toSx(gx),H-pad);ctx.stroke();
  }
  for(let gy=yMin;gy<=yMax;gy+=0.5){
    ctx.beginPath();ctx.moveTo(pad,toSy(gy));ctx.lineTo(W-pad,toSy(gy));ctx.stroke();
  }
  // axes
  ctx.strokeStyle='#2a2a2a'; ctx.lineWidth=1.5;
  ctx.beginPath();ctx.moveTo(pad,toSy(0));ctx.lineTo(W-pad,toSy(0));ctx.stroke();
  ctx.beginPath();ctx.moveTo(toSx(0),pad);ctx.lineTo(toSx(0),H-pad);ctx.stroke();
  // functions
  fns.forEach(fn=>{
    ctx.beginPath();ctx.strokeStyle=fn.color;ctx.lineWidth=fn.w||2;
    for(let px=0;px<=W-2*pad;px++){
      const x=xMin+px/(W-2*pad)*(xMax-xMin);
      const y=fn.f(x);
      const sy=toSy(Math.max(yMin,Math.min(yMax,y)));
      px===0?ctx.moveTo(toSx(x),sy):ctx.lineTo(toSx(x),sy);
    }
    ctx.stroke();
  });
}

function drawAll(){
  drawFn('cvSig',[{f:sigmoid,color:'#00ffe9',w:2},{f:sigmoidD,color:'#ffe900',w:1.5}],-0.2,1.3);
  drawFn('cvRelu',[{f:relu,color:'#ff2d55',w:2},{f:reluD,color:'#ffe900',w:1.5}],-0.3,3);
  drawFn('cvTanh',[{f:tanhF,color:'#ffe900',w:2},{f:tanhD,color:'#00ffe9',w:1.5}],-1.3,1.3);
  drawFn('cvGelu',[{f:gelu,color:'#e900ff',w:2},{f:geluD,color:'#ffe900',w:1.5}],-0.5,3);
}
window.addEventListener('load',drawAll);
window.addEventListener('resize',drawAll);

// gradient decay bars
const gradFns = {
  sigmoid: x=>sigmoidD(x),
  relu: x=>reluD(x),
  tanh: x=>tanhD(x),
  leaky: x=>x>0?1:0.01
};
// approximate per-layer multiplier for different activations
const gradMultiplier = {
  sigmoid: 0.25,
  relu: 0.9,      // ~90% survive (some dead neurons)
  tanh: 0.7,
  leaky: 0.95
};
const gradColor = {
  sigmoid:'#00ffe9', relu:'#ff2d55', tanh:'#ffe900', leaky:'#ff6b00'
};

let activeFn='sigmoid';

function buildBars(fn, layers){
  const container=document.getElementById('gradBars');
  container.innerHTML='';
  let g=1.0;
  const mult=gradMultiplier[fn];
  for(let i=0;i<layers;i++){
    g*=mult;
    const pct=Math.min(100,g*100);
    const row=document.createElement('div');
    row.style.cssText='display:flex;align-items:center;gap:8px;margin-bottom:4px;';
    row.innerHTML=`
      <span style="font-size:9px;color:#333;width:50px;text-align:right;flex-shrink:0;">layer ${i+1}</span>
      <div style="flex:1;height:14px;background:#0a0a0a;border:1px solid #161616;position:relative;">
        <div style="position:absolute;top:0;left:0;height:100%;width:${pct}%;background:${gradColor[fn]};opacity:0.8;"></div>
      </div>
      <span style="font-size:9px;width:70px;color:${g<0.001?'var(--red)':'#555'};">${g<0.0001?g.toExponential(2):g.toFixed(4)}</span>
    `;
    container.appendChild(row);
  }
  document.getElementById('finalGrad').textContent=g<0.0001?g.toExponential(2):g.toFixed(6);
  document.getElementById('finalGrad').style.color=g<0.01?'var(--red)':g<0.1?'var(--yellow)':'var(--green)';
}

document.querySelectorAll('#actBtns button').forEach(btn=>{
  btn.addEventListener('click',()=>{
    document.querySelectorAll('#actBtns button').forEach(b=>b.classList.remove('act'));
    btn.classList.add('act');
    activeFn=btn.dataset.fn;
    buildBars(activeFn,parseInt(document.getElementById('layerSlider').value));
  });
});
document.getElementById('layerSlider').addEventListener('input',e=>{
  document.getElementById('layerCount').textContent=e.target.value;
  buildBars(activeFn,parseInt(e.target.value));
});

// live output slider
document.getElementById('xSlider').addEventListener('input',e=>{
  const x=parseFloat(e.target.value);
  document.getElementById('xVal').textContent=x.toFixed(2);
  document.getElementById('outSig').textContent=sigmoid(x).toFixed(4);
  document.getElementById('gradSig').textContent=sigmoidD(x).toFixed(4);
  document.getElementById('outRelu').textContent=relu(x).toFixed(4);
  document.getElementById('gradRelu').textContent=reluD(x).toFixed(1);
  document.getElementById('outTanh').textContent=tanhF(x).toFixed(4);
  document.getElementById('gradTanh').textContent=tanhD(x).toFixed(4);
  document.getElementById('outGelu').textContent=gelu(x).toFixed(4);
  document.getElementById('gradGelu').textContent=geluD(x).toFixed(4);
});

window.addEventListener('load',()=>buildBars('sigmoid',10));
</script>
</body>
</html>
