<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Attention Mechanism</title>
<style>
@import url('https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&family=Bebas+Neue&display=swap');

:root {
  --bg: #0a0a0f;
  --panel: #0f0f1a;
  --panel2: #12121f;
  --border: #1e1e3a;
  --border2: #2a2a4a;
  --q: #ffaa00;
  --k: #00e5ff;
  --v: #69ff6e;
  --attn: #ff3d71;
  --out: #b060ff;
  --text: #c8c8e0;
  --dim: #555570;
  --white: #eeeeff;
}

* { box-sizing: border-box; margin: 0; padding: 0; }

body {
  background: var(--bg);
  color: var(--text);
  font-family: 'Space Mono', monospace;
  min-height: 100vh;
  overflow-x: hidden;
}

.noise {
  position: fixed; inset: 0; opacity: 0.035; pointer-events: none; z-index: 1000;
  background-image: url("data:image/svg+xml,%3Csvg viewBox='0 0 256 256' xmlns='http://www.w3.org/2000/svg'%3E%3Cfilter id='n'%3E%3CfeTurbulence type='fractalNoise' baseFrequency='0.9' numOctaves='4' stitchTiles='stitch'/%3E%3C/filter%3E%3Crect width='100%25' height='100%25' filter='url(%23n)'/%3E%3C/svg%3E");
}

header {
  padding: 3rem 2.5rem 2rem;
  border-bottom: 1px solid var(--border);
  position: relative; overflow: hidden;
}

header::before {
  content: 'ATTN';
  position: absolute; right: -0.5rem; top: 50%; transform: translateY(-50%);
  font-family: 'Bebas Neue', sans-serif; font-size: 16rem;
  color: var(--border); pointer-events: none; line-height: 1;
}

.doc-meta {
  font-size: 0.6rem; letter-spacing: 0.15em; text-transform: uppercase;
  color: var(--dim); margin-bottom: 1rem; display: flex; gap: 2rem; flex-wrap: wrap;
}

h1 {
  font-family: 'Bebas Neue', sans-serif;
  font-size: clamp(3rem, 9vw, 7.5rem);
  line-height: 0.88; letter-spacing: 0.01em; color: var(--white);
}
h1 em { color: var(--attn); font-style: normal; }

.subtitle { margin-top: 1rem; font-size: 0.7rem; color: var(--dim); max-width: 620px; line-height: 1.7; }

.section { padding: 2rem 2.5rem; border-bottom: 1px solid var(--border); }
.section-num { font-size: 0.55rem; letter-spacing: 0.2em; text-transform: uppercase; color: var(--dim); margin-bottom: 0.3rem; }
.section-title { font-family: 'Bebas Neue', sans-serif; font-size: 1.8rem; letter-spacing: 0.04em; color: var(--white); margin-bottom: 1rem; }
.prose { font-size: 0.72rem; line-height: 1.8; color: var(--text); max-width: 680px; }
.annotation { margin-top: 0.75rem; padding-left: 1rem; border-left: 2px solid var(--attn); font-size: 0.68rem; font-style: italic; color: var(--dim); }

/* QKV explainer */
.qkv-grid {
  display: grid;
  grid-template-columns: repeat(3, 1fr);
  gap: 1px;
  background: var(--border);
  margin-top: 1.25rem;
}
@media (max-width: 600px) { .qkv-grid { grid-template-columns: 1fr; } }

.qkv-card {
  background: var(--panel);
  padding: 1.25rem;
  position: relative;
}

.qkv-letter {
  font-family: 'Bebas Neue', sans-serif;
  font-size: 5rem;
  line-height: 0.85;
  letter-spacing: 0.02em;
  margin-bottom: 0.5rem;
}
.qkv-letter.q { color: var(--q); }
.qkv-letter.k { color: var(--k); }
.qkv-letter.v { color: var(--v); }

.qkv-name { font-size: 0.7rem; font-weight: 700; color: var(--white); margin-bottom: 0.5rem; letter-spacing: 0.05em; }
.qkv-def { font-size: 0.65rem; line-height: 1.7; color: var(--text); }
.qkv-formula {
  margin-top: 0.75rem; padding: 0.4rem 0.6rem;
  border-left: 2px solid var(--border2);
  font-size: 0.62rem; font-style: italic; line-height: 1.6;
}
.qkv-formula.q { color: var(--q); border-left-color: var(--q); }
.qkv-formula.k { color: var(--k); border-left-color: var(--k); }
.qkv-formula.v { color: var(--v); border-left-color: var(--v); }

/* attention visualization */
.attn-demo {
  margin-top: 1.25rem;
  border: 1px solid var(--border);
  background: var(--panel);
  overflow: hidden;
}

.attn-demo-head {
  background: var(--border2); color: var(--dim);
  font-size: 0.55rem; letter-spacing: 0.15em; text-transform: uppercase;
  padding: 0.35rem 0.75rem; border-bottom: 1px solid var(--border);
  display: flex; justify-content: space-between; align-items: center;
}

.token-sentence {
  display: flex;
  gap: 1px;
  background: var(--border);
  padding: 0;
  margin-bottom: 1px;
}

.token {
  flex: 1;
  padding: 0.65rem 0.4rem;
  text-align: center;
  font-size: 0.7rem;
  font-weight: 700;
  background: var(--panel2);
  cursor: pointer;
  transition: all 0.15s;
  border: 2px solid transparent;
  color: var(--text);
  letter-spacing: 0.03em;
  user-select: none;
}

.token:hover { background: var(--border2); }
.token.selected { border-color: var(--attn); color: var(--white); background: rgba(255,61,113,0.1); }

.attn-heatmap {
  display: flex;
  gap: 1px;
  background: var(--border);
  min-height: 60px;
  align-items: stretch;
}

.attn-cell {
  flex: 1;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  gap: 0.2rem;
  padding: 0.5rem 0.2rem;
  background: var(--panel);
  transition: background 0.3s;
  min-height: 60px;
}

.attn-val {
  font-family: 'Bebas Neue', sans-serif;
  font-size: 1.2rem;
  letter-spacing: 0.03em;
  transition: color 0.3s;
}

.attn-bar {
  width: 80%;
  height: 4px;
  background: var(--border2);
  border-radius: 0;
}
.attn-bar-fill {
  height: 100%;
  background: var(--attn);
  transition: width 0.4s cubic-bezier(0.25,0.46,0.45,0.94);
  box-shadow: 0 0 6px var(--attn);
}

.attn-label { font-size: 0.55rem; color: var(--dim); letter-spacing: 0.05em; }

.selected-info {
  padding: 0.75rem;
  font-size: 0.65rem;
  color: var(--dim);
  border-top: 1px solid var(--border);
  min-height: 36px;
}

/* formula */
.formula-section {
  margin-top: 1.25rem;
}

.big-formula {
  padding: 1.25rem;
  background: var(--panel);
  border: 1px solid var(--border);
  font-size: 0.9rem;
  letter-spacing: 0.02em;
  font-style: italic;
  text-align: center;
  color: var(--white);
  line-height: 2;
}

.big-formula .q { color: var(--q); }
.big-formula .k { color: var(--k); }
.big-formula .v { color: var(--v); }
.big-formula .d { color: var(--dim); }
.big-formula .softmax { color: var(--attn); }

.formula-breakdown {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
  gap: 1px;
  background: var(--border);
  margin-top: 1px;
}

.fbd-item {
  background: var(--panel2);
  padding: 0.75rem;
  font-size: 0.65rem;
  line-height: 1.6;
  color: var(--text);
}
.fbd-item strong { display: block; margin-bottom: 0.2rem; }

/* multi-head canvas */
canvas#mhCanvas {
  width: 100%;
  height: 280px;
  display: block;
  background: var(--panel);
  border: 1px solid var(--border);
  margin-top: 1.25rem;
}

/* transformer pipeline */
.pipeline {
  display: flex;
  gap: 1px;
  background: var(--border);
  margin-top: 1.25rem;
  overflow-x: auto;
}

.pipe-stage {
  flex: 1;
  min-width: 110px;
  background: var(--panel);
  padding: 1rem 0.75rem;
  text-align: center;
  position: relative;
}

.pipe-icon {
  font-family: 'Bebas Neue', sans-serif;
  font-size: 1.8rem;
  margin-bottom: 0.3rem;
  line-height: 1;
}
.pipe-name { font-size: 0.6rem; font-weight: 700; color: var(--white); letter-spacing: 0.05em; margin-bottom: 0.3rem; }
.pipe-desc { font-size: 0.58rem; color: var(--dim); line-height: 1.5; }

.pipe-arrow {
  position: absolute; right: -8px; top: 50%;
  transform: translateY(-50%);
  color: var(--dim); font-size: 0.8rem; z-index: 1;
}

/* concepts */
.concepts {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(230px, 1fr));
  gap: 1px; background: var(--border);
  border: 1px solid var(--border); margin-top: 1.5rem;
}

.concept-card { background: var(--panel); padding: 1.25rem; }
.concept-name { font-family: 'Bebas Neue', sans-serif; font-size: 1.15rem; letter-spacing: 0.04em; margin-bottom: 0.5rem; }
.concept-name.red { color: var(--attn); }
.concept-name.cyan { color: var(--k); }
.concept-name.green { color: var(--v); }
.concept-name.orange { color: var(--q); }
.concept-name.purple { color: var(--out); }
.concept-def { font-size: 0.65rem; line-height: 1.7; color: var(--text); }
.formula-block { margin-top: 0.6rem; padding: 0.4rem 0.6rem; border-left: 2px solid var(--border2); font-size: 0.62rem; font-style: italic; color: var(--k); background: var(--bg); }

footer {
  padding: 1.25rem 2.5rem; border-top: 1px solid var(--border);
  font-size: 0.55rem; color: var(--dim); letter-spacing: 0.1em; text-transform: uppercase;
  display: flex; justify-content: space-between; flex-wrap: wrap; gap: 0.5rem;
}

button {
  font-family: 'Space Mono', monospace;
  font-size: 0.6rem; letter-spacing: 0.1em; text-transform: uppercase;
  padding: 0.5rem 1.2rem; border: none;
  background: var(--panel2); color: var(--dim); cursor: pointer; transition: all 0.12s;
}
button:hover { background: var(--border2); color: var(--white); }
</style>
</head>
<body>
<div class="noise"></div>

<header>
  <div class="doc-meta">
    <span>Transformer Architecture</span>
    <span>Self-Attention Mechanism</span>
    <span>Series: AI Fundamentals</span>
  </div>
  <h1>SELF-<em>ATTEN</em><br>TION</h1>
  <p class="subtitle">The mechanism that lets a model look at every other word when processing each word — and decide dynamically how much to care about each one. This is what makes transformers work.</p>
</header>

<div class="section">
  <div class="section-num">01 / The Core Idea</div>
  <div class="section-title">WHAT PROBLEM DOES THIS SOLVE?</div>
  <p class="prose">
    Older sequence models (RNNs, LSTMs) processed tokens one at a time, left to right. Context from early in a sentence had to survive a long chain of transformations to influence tokens at the end — it often didn't. Attention breaks this constraint entirely.
  </p>
  <p class="prose" style="margin-top:0.75rem;">
    In self-attention, every token in a sequence can directly attend to every other token in a single step. When processing the word "it" in <em>"The animal didn't cross the street because it was too tired"</em> — attention lets the model directly look back at "animal" and "street" and compute which one "it" refers to, all in parallel.
  </p>
  <p class="prose" style="margin-top:0.75rem;">
    The mechanism is essentially a differentiable, learned database lookup. You submit a <strong style="color:var(--q)">Query</strong>, match it against a set of <strong style="color:var(--k)">Keys</strong>, retrieve a weighted sum of <strong style="color:var(--v)">Values</strong>. The weights are the attention scores — how relevant each token is to the current one.
  </p>
  <p class="annotation">
    "Attention Is All You Need" — Vaswani et al., 2017. The paper that replaced recurrence with pure attention and became the foundation for every LLM that exists today.
  </p>
</div>

<div class="section">
  <div class="section-num">02 / The Three Vectors</div>
  <div class="section-title">QUERY, KEY, VALUE</div>
  <p class="prose">
    For each token, three vectors are produced by multiplying the token's embedding by three learned weight matrices. These projections happen simultaneously for every token in the sequence.
  </p>
  <div class="qkv-grid">
    <div class="qkv-card">
      <div class="qkv-letter q">Q</div>
      <div class="qkv-name">Query</div>
      <div class="qkv-def">What am I looking for? The query represents the current token's "question" — what context does it need to be understood correctly? Think of it as a search term.</div>
      <div class="qkv-formula q">Q = X · W_Q</div>
    </div>
    <div class="qkv-card">
      <div class="qkv-letter k">K</div>
      <div class="qkv-name">Key</div>
      <div class="qkv-def">What do I contain? The key represents each token's "label" — what information it offers to other tokens that are looking. The dot product Q·K measures relevance.</div>
      <div class="qkv-formula k">K = X · W_K</div>
    </div>
    <div class="qkv-card">
      <div class="qkv-letter v">V</div>
      <div class="qkv-name">Value</div>
      <div class="qkv-def">What do I actually contribute? If a token is deemed relevant by the attention score, its value vector is what gets added to the output. The value can differ from what the key advertises.</div>
      <div class="qkv-formula v">V = X · W_V</div>
    </div>
  </div>
</div>

<div class="section">
  <div class="section-num">03 / Interactive Demo</div>
  <div class="section-title">ATTENTION IN ACTION</div>
  <p class="prose" style="margin-bottom:1rem;">
    Click any token below. See how much attention it pays to every other token in the sentence. These weights are computed scores — higher means "I drew more information from that token." The values shown are illustrative but reflect real linguistic patterns a trained model would learn.
  </p>
  <div class="attn-demo">
    <div class="attn-demo-head">
      <span>Self-Attention Visualization</span>
      <span id="selectedTokenLabel" style="color:var(--attn);">Select a token</span>
    </div>
    <div class="token-sentence" id="tokenRow"></div>
    <div class="attn-heatmap" id="attnHeatmap"></div>
    <div class="selected-info" id="selectedInfo">Click a word above to see its attention distribution across the sequence.</div>
  </div>
</div>

<div class="section">
  <div class="section-num">04 / The Formula</div>
  <div class="section-title">SCALED DOT-PRODUCT ATTENTION</div>
  <p class="prose" style="margin-bottom:1rem;">
    The full computation in one equation. The scaling factor √d_k prevents dot products from growing so large that softmax saturates, which would make gradients vanishingly small.
  </p>
  <div class="big-formula">
    Attention(<span class="q">Q</span>, <span class="k">K</span>, <span class="v">V</span>) = <span class="softmax">softmax</span>( <span class="q">Q</span><span class="k">K</span><sup>T</sup> / <span class="d">√d_k</span> ) <span class="v">V</span>
  </div>
  <div class="formula-breakdown">
    <div class="fbd-item"><strong style="color:var(--q);">Q·K^T</strong>Raw similarity scores between every query-key pair. Produces an N×N matrix for a sequence of length N. Every token against every other token.</div>
    <div class="fbd-item"><strong style="color:var(--dim);">/ √d_k</strong>Scaling to stabilize gradients. Without this, dot products grow proportionally to embedding dimension d_k, pushing softmax into saturation zones where gradients die.</div>
    <div class="fbd-item"><strong style="color:var(--attn);">softmax( )</strong>Converts raw scores to a probability distribution over the sequence. Each row sums to 1. High scores become close to 1; low scores collapse toward 0.</div>
    <div class="fbd-item"><strong style="color:var(--v);">× V</strong>Weighted sum of value vectors. Each output token is a blend of all value vectors, weighted by how much attention was paid to each source token.</div>
  </div>
</div>

<div class="section">
  <div class="section-num">05 / Multi-Head Attention</div>
  <div class="section-title">MULTIPLE ATTENTION HEADS</div>
  <p class="prose">
    A single attention head learns one type of relationship. But language has many kinds simultaneously — syntactic dependencies, coreference, semantic similarity, positional proximity. Multi-head attention runs several attention operations in parallel, each with its own learned Q/K/V projections, then concatenates the results.
  </p>
  <p class="prose" style="margin-top:0.75rem;">
    In GPT-3, each layer has 96 heads across a 12288-dimensional embedding space — each head operates in a 128-dimensional subspace. Interpretability research has found that specific heads specialize: some track subject-verb agreement, some follow coreference chains, some handle positional patterns. The specialization emerges from training, not design.
  </p>
  <canvas id="mhCanvas"></canvas>
</div>

<div class="section">
  <div class="section-num">06 / The Full Stack</div>
  <div class="section-title">WHERE ATTENTION LIVES IN A TRANSFORMER</div>
  <p class="prose" style="margin-bottom:1rem;">
    Attention is one piece inside a transformer block. Each block stacks these operations, and a full model stacks many blocks. GPT-4 reportedly has ~120 layers of this.
  </p>
  <div class="pipeline">
    <div class="pipe-stage">
      <div class="pipe-icon" style="color:var(--dim);">T</div>
      <div class="pipe-name">Token Embed</div>
      <div class="pipe-desc">Raw tokens → dense vectors via embedding table</div>
      <div class="pipe-arrow">›</div>
    </div>
    <div class="pipe-stage">
      <div class="pipe-icon" style="color:var(--dim);">P</div>
      <div class="pipe-name">Pos. Encoding</div>
      <div class="pipe-desc">Add positional signal — attention itself is order-agnostic</div>
      <div class="pipe-arrow">›</div>
    </div>
    <div class="pipe-stage">
      <div class="pipe-icon" style="color:var(--attn);">A</div>
      <div class="pipe-name">Multi-Head Attn</div>
      <div class="pipe-desc">Parallel Q/K/V projections, scaled dot-product, concat</div>
      <div class="pipe-arrow">›</div>
    </div>
    <div class="pipe-stage">
      <div class="pipe-icon" style="color:var(--dim);">N</div>
      <div class="pipe-name">Layer Norm</div>
      <div class="pipe-desc">Stabilize activations. Applied before or after attention</div>
      <div class="pipe-arrow">›</div>
    </div>
    <div class="pipe-stage">
      <div class="pipe-icon" style="color:var(--out);">F</div>
      <div class="pipe-name">FFN</div>
      <div class="pipe-desc">Two linear layers with ReLU/GELU. Expands then contracts. Stores factual knowledge</div>
      <div class="pipe-arrow">›</div>
    </div>
    <div class="pipe-stage">
      <div class="pipe-icon" style="color:var(--dim);">+</div>
      <div class="pipe-name">Residual</div>
      <div class="pipe-desc">Skip connections around each sub-layer. Critical for gradient flow through depth</div>
      <div class="pipe-arrow">›</div>
    </div>
    <div class="pipe-stage">
      <div class="pipe-icon" style="color:var(--v);">×N</div>
      <div class="pipe-name">Repeat</div>
      <div class="pipe-desc">Stack N identical blocks. Each layer builds on the previous representation</div>
    </div>
  </div>
</div>

<div class="section">
  <div class="section-num">07 / Concepts</div>
  <div class="section-title">CORE VOCABULARY</div>
  <div class="concepts">
    <div class="concept-card">
      <div class="concept-name orange">Attention Score</div>
      <div class="concept-def">The raw similarity between a query and a key, computed as a dot product. Before softmax. Higher means "this token is more relevant to what I'm looking for." Gets scaled by √d_k before normalization.</div>
      <div class="formula-block">score(Q,K) = Q·K^T / √d_k</div>
    </div>
    <div class="concept-card">
      <div class="concept-name cyan">Attention Weight</div>
      <div class="concept-def">The attention score after passing through softmax. Now a probability distribution summing to 1. This weight determines how much of each value vector contributes to the output for the current query token.</div>
    </div>
    <div class="concept-card">
      <div class="concept-name red">Causal Masking</div>
      <div class="concept-def">In language models, a token shouldn't be able to attend to tokens that come after it — that would be cheating during training. A mask sets future positions to -∞ before softmax, collapsing their weight to zero. This is what makes GPT "autoregressive."</div>
    </div>
    <div class="concept-card">
      <div class="concept-name purple">Context Window</div>
      <div class="concept-def">The maximum sequence length the model can attend over. Since attention is O(n²) in sequence length — every token attends to every other — longer contexts get expensive fast. GPT-4 Turbo: 128k tokens. Modern research pushes toward millions.</div>
    </div>
    <div class="concept-card">
      <div class="concept-name green">Residual Stream</div>
      <div class="concept-def">The main information highway running through all layers. Each attention and FFN block adds its output to the residual stream rather than replacing it. Interpretability research treats this as the primary object that layers read from and write to.</div>
    </div>
    <div class="concept-card">
      <div class="concept-name orange">Positional Encoding</div>
      <div class="concept-def">Attention is permutation-invariant — "the cat sat" and "sat cat the" look identical to a pure attention mechanism. Positional encodings inject sequence order. Original transformers used fixed sinusoidal encodings; modern models use learned or rotary (RoPE) encodings.</div>
    </div>
    <div class="concept-card">
      <div class="concept-name cyan">KV Cache</div>
      <div class="concept-def">During inference, Key and Value matrices for previously generated tokens don't change. KV caching stores them so they don't need to be recomputed each step. Without this, autoregressive generation would be O(n²) per token instead of O(n).</div>
    </div>
    <div class="concept-card">
      <div class="concept-name red">Head Specialization</div>
      <div class="concept-def">Individual attention heads learn distinct patterns. Interpretability work (Anthropic, Harvard NLP) has identified heads for: induction (copying patterns), name lookup, syntactic structure, and more. These emerge from training without explicit design.</div>
    </div>
  </div>
</div>

<footer>
  <span>Attention Is All You Need — Vaswani et al. (2017)</span>
  <span>AI Fundamentals Series</span>
  <span>Self-Attention / Transformer Architecture</span>
</footer>

<script>
// ===== ATTENTION DEMO =====
const sentence = ['The', 'animal', 'did', 'not', 'cross', 'the', 'street', 'because', 'it', 'was', 'tired'];

// Plausible attention patterns per token (illustrative, linguistically motivated)
const attnWeights = {
  'The':     [0.45, 0.20, 0.05, 0.02, 0.05, 0.10, 0.05, 0.02, 0.03, 0.02, 0.01],
  'animal':  [0.15, 0.35, 0.04, 0.03, 0.08, 0.06, 0.04, 0.05, 0.12, 0.05, 0.03],
  'did':     [0.06, 0.08, 0.30, 0.15, 0.12, 0.05, 0.06, 0.07, 0.04, 0.04, 0.03],
  'not':     [0.04, 0.05, 0.22, 0.28, 0.16, 0.04, 0.05, 0.07, 0.04, 0.03, 0.02],
  'cross':   [0.05, 0.10, 0.08, 0.08, 0.28, 0.06, 0.10, 0.09, 0.06, 0.04, 0.06],
  'the':     [0.25, 0.08, 0.03, 0.02, 0.05, 0.30, 0.16, 0.04, 0.03, 0.02, 0.02],
  'street':  [0.04, 0.06, 0.05, 0.04, 0.18, 0.12, 0.34, 0.08, 0.03, 0.03, 0.03],
  'because': [0.04, 0.08, 0.06, 0.08, 0.10, 0.04, 0.08, 0.28, 0.12, 0.06, 0.06],
  'it':      [0.04, 0.32, 0.03, 0.03, 0.05, 0.03, 0.10, 0.10, 0.18, 0.06, 0.06],
  'was':     [0.03, 0.10, 0.05, 0.04, 0.05, 0.03, 0.05, 0.10, 0.24, 0.22, 0.09],
  'tired':   [0.03, 0.18, 0.04, 0.03, 0.05, 0.03, 0.06, 0.08, 0.20, 0.18, 0.12],
};

// Normalize so rows sum to 1
Object.keys(attnWeights).forEach(k => {
  const row = attnWeights[k];
  const sum = row.reduce((a,b) => a+b, 0);
  attnWeights[k] = row.map(v => v/sum);
});

let selectedToken = null;
const tokenRow = document.getElementById('tokenRow');
const attnHeatmap = document.getElementById('attnHeatmap');

// build token buttons
sentence.forEach((word, i) => {
  const el = document.createElement('div');
  el.className = 'token';
  el.textContent = word;
  el.addEventListener('click', () => selectToken(i, el));
  tokenRow.appendChild(el);
});

// build heatmap cells
sentence.forEach((word, i) => {
  const cell = document.createElement('div');
  cell.className = 'attn-cell';
  cell.id = `attn-cell-${i}`;
  cell.innerHTML = `
    <div class="attn-val" id="attn-val-${i}">—</div>
    <div class="attn-bar"><div class="attn-bar-fill" id="attn-bar-${i}" style="width:0%"></div></div>
    <div class="attn-label">${word}</div>
  `;
  attnHeatmap.appendChild(cell);
});

function selectToken(idx, el) {
  // deselect all
  document.querySelectorAll('.token').forEach(t => t.classList.remove('selected'));
  el.classList.add('selected');
  selectedToken = idx;
  const word = sentence[idx];
  const weights = attnWeights[word];

  document.getElementById('selectedTokenLabel').textContent = `"${word}" attending to:`;

  weights.forEach((w, i) => {
    const pct = (w * 100).toFixed(0);
    const valEl = document.getElementById(`attn-val-${i}`);
    const barEl = document.getElementById(`attn-bar-${i}`);
    const cellEl = document.getElementById(`attn-cell-${i}`);

    valEl.textContent = (w * 100).toFixed(1) + '%';
    // color by intensity
    const alpha = 0.1 + w * 2.5;
    valEl.style.color = `rgba(255, 61, 113, ${Math.min(alpha, 1)})`;
    barEl.style.width = (w * 100 * 3.5) + '%'; // amplified for visual
    cellEl.style.background = `rgba(255, 61, 113, ${Math.min(w * 1.5, 0.25)})`;
  });

  // find top attended
  const maxW = Math.max(...weights);
  const maxIdx = weights.indexOf(maxW);
  const info = `"${word}" attends most strongly to "${sentence[maxIdx]}" (${(maxW*100).toFixed(1)}%). ` +
    (word === 'it' ? 'Classic coreference — the model has learned to link the pronoun back to its referent.' :
     word === 'tired' ? 'Semantic anchor — "it was tired" pulls attention back to the subject "animal."' :
     word === 'not' ? 'Negation token strongly attends to the verb it modifies — "did not cross."' :
     word === 'because' ? 'Causal connector attends to both the cause and the clause being caused.' :
     'Each head independently learns which relationships matter for accurate next-token prediction.');
  document.getElementById('selectedInfo').textContent = info;
}

// ===== MULTI-HEAD CANVAS =====
const mhc = document.getElementById('mhCanvas');
const mhctx = mhc.getContext('2d');

const HEAD_COLORS = [
  '#ff3d71','#00e5ff','#69ff6e','#ffaa00','#b060ff','#ff8c00',
  '#00bcd4','#e91e63'
];
const HEAD_LABELS = ['syntax','coref','semantic','position','negation','copy','entity','temporal'];

function drawMH() {
  const dpr = window.devicePixelRatio || 1;
  const rect = mhc.getBoundingClientRect();
  mhc.width = rect.width * dpr;
  mhc.height = rect.height * dpr;
  mhctx.scale(dpr, dpr);
  const W = rect.width, H = rect.height;

  mhctx.fillStyle = '#0f0f1a';
  mhctx.fillRect(0, 0, W, H);

  const numHeads = 8;
  const tokens = ['The', 'cat', 'sat', 'on', 'the', 'mat'];
  const nT = tokens.length;
  const headH = (H - 40) / numHeads;
  const tokenW = (W - 130) / nT;
  const offsetX = 120;
  const offsetY = 20;

  // token headers
  mhctx.fillStyle = '#555570';
  mhctx.font = '8px Space Mono';
  mhctx.textAlign = 'center';
  tokens.forEach((t, i) => {
    mhctx.fillText(t, offsetX + i * tokenW + tokenW/2, 14);
  });

  // draw each head
  for (let h = 0; h < numHeads; h++) {
    const y = offsetY + h * headH;
    const col = HEAD_COLORS[h];

    // head label
    mhctx.fillStyle = col;
    mhctx.font = 'bold 8px Space Mono';
    mhctx.textAlign = 'right';
    mhctx.fillText(`head ${h+1}`, 68, y + headH/2 + 3);
    mhctx.fillStyle = '#2a2a4a';
    mhctx.font = '7px Space Mono';
    mhctx.fillText(HEAD_LABELS[h], 114, y + headH/2 + 3);

    // generate a plausible attention pattern for this head
    for (let qi = 0; qi < nT; qi++) {
      for (let ki = 0; ki < nT; ki++) {
        // different patterns per head type
        let w = 0.05;
        if (HEAD_LABELS[h] === 'syntax') {
          // attend to neighbors
          w = Math.exp(-Math.abs(qi-ki)*1.2) * 0.5 + 0.02;
        } else if (HEAD_LABELS[h] === 'coref') {
          // "the" tokens attend to each other
          w = (tokens[qi]==='the'||tokens[qi]==='The') && (tokens[ki]==='the'||tokens[ki]==='The') ? 0.6 : 0.05;
          if (qi===ki) w += 0.1;
        } else if (HEAD_LABELS[h] === 'semantic') {
          // cat-mat-sat cluster
          const sem = ['cat','sat','mat'];
          w = sem.includes(tokens[qi]) && sem.includes(tokens[ki]) ? 0.4 : 0.05;
          if(qi===ki) w += 0.1;
        } else if (HEAD_LABELS[h] === 'position') {
          // diagonal + previous token
          w = qi===ki ? 0.5 : (ki===qi-1 ? 0.3 : 0.03);
        } else if (HEAD_LABELS[h] === 'negation') {
          // attend to first token
          w = ki===0 ? 0.4 : (qi===ki ? 0.2 : 0.05);
        } else if (HEAD_LABELS[h] === 'copy') {
          // strong diagonal
          w = qi===ki ? 0.8 : 0.04;
        } else if (HEAD_LABELS[h] === 'entity') {
          // "cat" and "mat" as entities
          w = (tokens[ki]==='cat'||tokens[ki]==='mat') ? 0.35 : 0.05;
        } else {
          // temporal: attend to earlier
          w = ki <= qi ? 0.15 + Math.random()*0.1 : 0.03;
        }

        const alpha = Math.min(w * 0.85, 0.85);
        const cx = offsetX + ki * tokenW + tokenW/2;
        const cy = y + headH/2;
        const r = Math.max(2, Math.min(headH * 0.38 * w * 2, headH * 0.42));

        mhctx.fillStyle = col.replace(')', `, ${alpha})`).replace('rgb', 'rgba').replace('#', '');
        // just draw as colored rect
        const hex = col;
        const rr = parseInt(hex.slice(1,3),16);
        const gg = parseInt(hex.slice(3,5),16);
        const bb = parseInt(hex.slice(5,7),16);
        mhctx.fillStyle = `rgba(${rr},${gg},${bb},${alpha})`;
        mhctx.fillRect(
          offsetX + ki * tokenW + 2,
          y + 2,
          tokenW - 4,
          headH - 4
        );
        // override alpha based on weight — redraw
        mhctx.fillStyle = `rgba(${rr},${gg},${bb},${alpha})`;
        // draw as circle instead
        mhctx.beginPath();
        mhctx.arc(
          offsetX + ki * tokenW + tokenW/2,
          y + headH/2,
          Math.max(1.5, Math.min(r, headH*0.45)),
          0, Math.PI*2
        );
        mhctx.fill();
      }
    }

    // row label (which query token we're looking at — use middle)
    const focusQ = Math.floor(nT/2);
  }

  // column lines
  mhctx.strokeStyle = '#1e1e3a';
  mhctx.lineWidth = 1;
  for (let i = 0; i <= nT; i++) {
    const x = offsetX + i * tokenW;
    mhctx.beginPath();
    mhctx.moveTo(x, offsetY);
    mhctx.lineTo(x, H - 10);
    mhctx.stroke();
  }

  // label
  mhctx.fillStyle = '#555570';
  mhctx.font = '8px Space Mono';
  mhctx.textAlign = 'left';
  mhctx.fillText('← Each row: one head. Each column: a token. Circle size = attention weight from that head to that token.', 10, H - 6);
}

// ===== INIT =====
window.addEventListener('load', () => { drawMH(); });
window.addEventListener('resize', () => { drawMH(); });
</script>
</body>
</html>
