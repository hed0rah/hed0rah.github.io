<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Attention Mechanism</title>
<style>
@import url('https://fonts.googleapis.com/css2?family=Space+Mono:ital,wght@0,400;0,700;1,400&family=Bebas+Neue&display=swap');
:root {
  --bg:        #0a0a0a;
  --surface:   #111111;
  --surface2:  #0d0d0d;
  --border:    #1e1e1e;
  --border2:   #2a2a2a;
  --cyan:      #00ffe9;
  --yellow:    #ffe900;
  --magenta:   #e900ff;
  --red:       #ff2d55;
  --green:     #39ff14;
  --orange:    #ff6b00;
  --dim:       #3a3a3a;
  --text:      #cccccc;
  --bright:    #ffffff;
  /* semantic aliases for this doc */
  --q:   #ffaa00;
  --k:   #00ffe9;
  --v:   #39ff14;
  --attn:#ff2d55;
  --out: #e900ff;
}
*{margin:0;padding:0;box-sizing:border-box;}
body{background:var(--bg);color:var(--text);font-family:'Space Mono',monospace;font-size:11px;line-height:1.65;padding:36px 30px;max-width:980px;margin:0 auto;}
body::before{content:'';position:fixed;inset:0;background:repeating-linear-gradient(0deg,transparent,transparent 2px,rgba(0,0,0,0.04) 2px,rgba(0,0,0,0.04) 4px);pointer-events:none;z-index:999;}

.masthead{border-top:3px solid var(--cyan);padding:14px 0 10px;margin-bottom:28px;display:flex;justify-content:space-between;align-items:flex-end;border-bottom:1px solid var(--border);}
.masthead h1{font-family:'Bebas Neue',sans-serif;font-size:64px;letter-spacing:4px;color:var(--bright);line-height:1;}
.masthead h1 em{color:var(--cyan);font-style:normal;}
.masthead-right{text-align:right;font-size:10px;color:#444;letter-spacing:2px;text-transform:uppercase;}
.masthead-right strong{display:block;color:var(--cyan);font-size:11px;margin-bottom:2px;}

.g2{display:grid;grid-template-columns:1fr 1fr;gap:2px;margin-bottom:2px;}
.g3{display:grid;grid-template-columns:1fr 1fr 1fr;gap:2px;margin-bottom:2px;}
.g4{display:grid;grid-template-columns:1fr 1fr 1fr 1fr;gap:2px;margin-bottom:2px;}
.gfull{margin-bottom:2px;}

.panel{background:var(--surface);border:1px solid var(--border);padding:18px;}
.plabel{font-family:'Bebas Neue',sans-serif;font-size:10px;letter-spacing:3px;color:#383838;text-transform:uppercase;margin-bottom:10px;padding-bottom:6px;border-bottom:1px solid var(--border);}
.ptitle{font-family:'Bebas Neue',sans-serif;font-size:26px;letter-spacing:2px;color:var(--bright);margin-bottom:8px;line-height:1;}
.ptitle.cy{color:var(--cyan);}.ptitle.rd{color:var(--red);}.ptitle.yl{color:var(--yellow);}.ptitle.pu{color:var(--magenta);}.ptitle.or{color:var(--orange);}.ptitle.gr{color:var(--green);}
.bar{width:28px;height:2px;margin-bottom:10px;background:var(--cyan);}
.bar.rd{background:var(--red);}.bar.yl{background:var(--yellow);}.bar.pu{background:var(--magenta);}.bar.or{background:var(--orange);}.bar.gr{background:var(--green);}

p{font-size:11px;color:var(--text);margin-bottom:7px;line-height:1.65;}
.hl{color:var(--cyan);font-weight:700;}.hl.rd{color:var(--red);}.hl.yl{color:var(--yellow);}.hl.pu{color:var(--magenta);}.hl.or{color:var(--orange);}.hl.gr{color:var(--green);}

.math-block{background:#060606;border:1px solid var(--border);border-left:3px solid var(--yellow);padding:12px 14px;margin:10px 0;font-size:11px;}
.math-block.cy{border-left-color:var(--cyan);}.math-block.rd{border-left-color:var(--red);}
.eq{color:var(--yellow);}.eq.cy{color:var(--cyan);}

.tag{display:inline-block;border:1px solid var(--border);padding:1px 6px;font-size:9px;letter-spacing:1px;text-transform:uppercase;color:#444;margin:2px 2px 2px 0;}
.tag.cy{border-color:var(--cyan);color:var(--cyan);}.tag.rd{border-color:var(--red);color:var(--red);}.tag.yl{border-color:var(--yellow);color:var(--yellow);}.tag.pu{border-color:var(--magenta);color:var(--magenta);}.tag.or{border-color:var(--orange);color:var(--orange);}.tag.gr{border-color:var(--green);color:var(--green);}

.org-row{display:flex;justify-content:space-between;padding:5px 0;border-bottom:1px solid #111;font-size:10px;}
.org-row:last-child{border-bottom:none;}
.org-row-label{color:#333;text-transform:uppercase;letter-spacing:1px;font-size:9px;}
.org-row-val{font-weight:700;}

.footer{border-top:1px solid var(--border);margin-top:14px;padding-top:10px;display:flex;justify-content:space-between;font-size:9px;color:#2a2a2a;letter-spacing:1px;}

button{font-family:'Space Mono',monospace;font-size:10px;letter-spacing:2px;text-transform:uppercase;padding:7px 12px;border:1px solid var(--border2);background:var(--surface2);color:var(--dim);cursor:pointer;transition:all 0.1s;}
button:hover{background:var(--border2);color:var(--bright);}
button.act{background:var(--cyan);color:var(--bg);border-color:var(--cyan);}

canvas{display:block;}
@media(max-width:700px){.g2,.g3,.g4{grid-template-columns:1fr;}}
</style>
</head>
<body>

<div class="masthead">
  <h1>ATTEN<em>TION</em><br>MECHANISM</h1>
  <div class="masthead-right">
    <strong>QUERY / KEY / VALUE — SCALED DOT-PRODUCT</strong>
    SOFTMAX ROUTING // MULTI-HEAD // SELF-ATTENTION<br>
    VASWANI ET AL. 2017 // TRANSFORMER CORE
  </div>
</div>

<!-- 01 CONCEPT -->
<div class="gfull">
  <div class="panel">
    <div class="plabel">01 // Concept</div>
    <div class="ptitle cy">WHAT ATTENTION DOES</div>
    <div class="bar"></div>
    <div style="display:grid;grid-template-columns:2fr 1fr;gap:20px;">
      <div>
        <p>Before attention, sequence models had to compress an entire input sequence into a single fixed-size context vector — a bottleneck that forced information loss. Attention broke this by letting every output position look back at <span class="hl">all input positions simultaneously</span>, weighting them by relevance.</p>
        <p>The core idea is a soft, differentiable dictionary lookup. A <span class="hl" style="color:var(--q);">query</span> is compared against a set of <span class="hl" style="color:var(--k);">keys</span>. The similarities become weights. Those weights aggregate over <span class="hl" style="color:var(--v);">values</span> to produce an output. Everything is continuous and therefore backpropagable — the network learns which queries, keys, and values to produce.</p>
        <p>Self-attention is the special case where queries, keys, and values all come from the same sequence. Each position asks: "which other positions are relevant to understanding me?" and the answer updates the representation. Stack enough of these with feedforward layers and you have a transformer.</p>
      </div>
      <div>
        <div style="display:grid;grid-template-columns:1fr 1fr;gap:8px;text-align:center;">
          <div><div style="font-family:'Bebas Neue',sans-serif;font-size:28px;color:var(--cyan);line-height:1;">2017</div><div style="font-size:9px;color:#444;letter-spacing:1px;text-transform:uppercase;">Vaswani<br>et al.</div></div>
          <div><div style="font-family:'Bebas Neue',sans-serif;font-size:28px;color:var(--yellow);line-height:1;">O(n²)</div><div style="font-size:9px;color:#444;letter-spacing:1px;text-transform:uppercase;">complexity<br>vs n</div></div>
          <div><div style="font-family:'Bebas Neue',sans-serif;font-size:28px;color:var(--green);line-height:1;">8</div><div style="font-size:9px;color:#444;letter-spacing:1px;text-transform:uppercase;">heads in<br>orig paper</div></div>
          <div><div style="font-family:'Bebas Neue',sans-serif;font-size:28px;color:var(--red);line-height:1;">√d</div><div style="font-size:9px;color:#444;letter-spacing:1px;text-transform:uppercase;">scale<br>factor</div></div>
        </div>
      </div>
    </div>
  </div>
</div>

<!-- 02 QKV -->
<div class="g3">
  <div class="panel">
    <div class="plabel">02 // Query</div>
    <div class="ptitle" style="color:var(--q);font-size:56px;line-height:0.9;margin-bottom:12px;">Q</div>
    <div class="bar" style="background:var(--q);"></div>
    <p><span class="hl" style="color:var(--q);">What am I looking for?</span> The query is a learned linear projection of the current token's representation. It encodes the "information need" of this position.</p>
    <div class="math-block" style="border-left-color:var(--q);"><span class="eq" style="color:var(--q);">Q = X · W_Q</span><br><span style="color:#444;font-size:10px;">W_Q ∈ R^{d_model × d_k} — learned</span></div>
    <p style="font-size:10px;color:#555;">In cross-attention (encoder-decoder), queries come from the decoder. In self-attention, all three come from the same source sequence X.</p>
  </div>
  <div class="panel">
    <div class="plabel">03 // Key</div>
    <div class="ptitle" style="color:var(--k);font-size:56px;line-height:0.9;margin-bottom:12px;">K</div>
    <div class="bar" style="background:var(--k);"></div>
    <p><span class="hl" style="color:var(--k);">What do I contain?</span> Keys are another linear projection of each token. They advertise content. A high dot product between a query and a key means that token is relevant to the current position's need.</p>
    <div class="math-block cy"><span class="eq cy">K = X · W_K</span><br><span style="color:#444;font-size:10px;">W_K ∈ R^{d_model × d_k} — learned</span></div>
    <p style="font-size:10px;color:#555;">Keys are compared against all queries — this is the O(n²) step. Every pair (i,j) requires a dot product. For n=4096 tokens, that's 16M operations per head per layer.</p>
  </div>
  <div class="panel">
    <div class="plabel">04 // Value</div>
    <div class="ptitle" style="color:var(--v);font-size:56px;line-height:0.9;margin-bottom:12px;">V</div>
    <div class="bar" style="background:var(--v);"></div>
    <p><span class="hl" style="color:var(--v);">What do I actually send?</span> Values are the third projection. Once attention weights are computed via Q·K, the output is a weighted sum of values. The keys determine relevance; values carry the actual information transferred.</p>
    <div class="math-block" style="border-left-color:var(--v);"><span class="eq" style="color:var(--v);">V = X · W_V</span><br><span style="color:#444;font-size:10px;">W_V ∈ R^{d_model × d_v} — learned</span></div>
    <p style="font-size:10px;color:#555;">Keys and values are often called "memories." The query retrieves from memory by matching keys, then reads out the associated values. Identical in structure to a Modern Hopfield network update.</p>
  </div>
</div>

<!-- 03 FORMULA -->
<div class="gfull">
  <div class="panel">
    <div class="plabel">05 // The Formula</div>
    <div class="ptitle yl">SCALED DOT-PRODUCT ATTENTION</div>
    <div class="bar yl"></div>
    <div style="text-align:center;padding:18px 0;font-family:'Bebas Neue',sans-serif;font-size:22px;letter-spacing:3px;line-height:2.2;border:1px solid var(--border);background:#060606;margin-bottom:2px;">
      <span style="color:var(--out);">Attention</span>(<span style="color:var(--q);">Q</span>, <span style="color:var(--k);">K</span>, <span style="color:var(--v);">V</span>) =
      <span style="color:var(--attn);">softmax</span><span style="color:#555;">(</span><span style="color:var(--q);">Q</span><span style="color:var(--k);">K</span><span style="color:#555;">ᵀ</span> / <span style="color:var(--yellow);">√d_k</span><span style="color:#555;">)</span>
      <span style="color:var(--v);">V</span>
    </div>
    <div style="display:grid;grid-template-columns:repeat(4,1fr);gap:1px;background:var(--border);">
      <div style="background:var(--surface2);padding:12px;">
        <div style="font-size:9px;color:var(--q);letter-spacing:2px;text-transform:uppercase;margin-bottom:4px;">QKᵀ</div>
        <p style="font-size:10px;color:#555;">Matrix multiply of queries by transposed keys. Produces an n×n similarity matrix — every pair of positions gets a raw score. Expensive but parallelizable.</p>
      </div>
      <div style="background:var(--surface2);padding:12px;">
        <div style="font-size:9px;color:var(--yellow);letter-spacing:2px;text-transform:uppercase;margin-bottom:4px;">/ √d_k</div>
        <p style="font-size:10px;color:#555;">Scale factor prevents dot products from growing too large in high dimensions (where variance scales with d_k). Without this, softmax saturates and gradients vanish — rediscovery of the Hopfield β parameter.</p>
      </div>
      <div style="background:var(--surface2);padding:12px;">
        <div style="font-size:9px;color:var(--attn);letter-spacing:2px;text-transform:uppercase;margin-bottom:4px;">softmax</div>
        <p style="font-size:10px;color:#555;">Applied row-wise. Converts raw scores to a probability distribution over positions. Each row sums to 1 — these are the attention weights. Differentiable everywhere.</p>
      </div>
      <div style="background:var(--surface2);padding:12px;">
        <div style="font-size:9px;color:var(--v);letter-spacing:2px;text-transform:uppercase;margin-bottom:4px;">·V</div>
        <p style="font-size:10px;color:#555;">Final weighted sum. Each output position gets a convex combination of all value vectors, weighted by how much attention it paid to each position. This is the output.</p>
      </div>
    </div>
  </div>
</div>

<!-- 04 INTERACTIVE ATTENTION HEATMAP -->
<div class="gfull">
  <div class="panel">
    <div class="plabel">06 // Interactive // Coreference Resolution</div>
    <div class="ptitle cy">CLICK A TOKEN TO SEE ITS ATTENTION</div>
    <div class="bar"></div>
    <p>Classic example from Vaswani et al. The pronoun "it" must resolve to either "animal" or "street." Select any token to see how it distributes attention weight across the sentence. The model learns these patterns from data — no explicit grammar rules.</p>

    <div style="background:#060606;border:1px solid var(--border);margin-top:12px;overflow:hidden;">
      <div style="background:var(--border);padding:5px 10px;font-size:9px;color:#333;letter-spacing:2px;text-transform:uppercase;display:flex;justify-content:space-between;">
        <span>Self-Attention // Sentence</span>
        <span id="selectedTokenLabel" style="color:var(--cyan);">— select a token —</span>
      </div>
      <div id="tokenRow" style="display:flex;gap:1px;background:var(--border);"></div>
      <div id="attnHeatmap" style="display:flex;gap:1px;background:var(--border);min-height:70px;align-items:stretch;"></div>
      <div id="selectedInfo" style="padding:10px 12px;font-size:10px;color:#444;border-top:1px solid var(--border);min-height:36px;line-height:1.6;"></div>
    </div>
  </div>
</div>

<!-- 05 MULTI-HEAD -->
<div class="gfull">
  <div class="panel">
    <div class="plabel">07 // Multi-Head Attention</div>
    <div class="ptitle cy">8 HEADS — 8 RELATIONSHIP TYPES</div>
    <div class="bar"></div>
    <p>Running attention once gives one perspective on which positions are relevant. Multi-head attention runs h independent attention operations in parallel — each with its own Q, K, V projections — then concatenates and projects the results. Each head can specialize in a different relationship type.</p>
    <div class="math-block"><span class="eq">MultiHead(Q,K,V) = Concat(head_1, ..., head_h) · W_O</span><br><span style="color:#444;font-size:10px;">head_i = Attention(Q·W_Qi, K·W_Ki, V·W_Vi)</span></div>
    <p>The visualization below shows 8 heads for the sentence "The cat sat on the mat." Circle size encodes attention weight from each head to each token. Note how different heads develop distinct, interpretable specializations — these emerge purely from gradient descent.</p>
    <canvas id="mhCanvas" style="width:100%;height:300px;background:#060606;border:1px solid var(--border);margin-top:12px;"></canvas>
    <div style="margin-top:8px;display:flex;gap:6px;flex-wrap:wrap;" id="headLegend"></div>
  </div>
</div>

<!-- 06 TYPES + COMPLEXITY -->
<div class="g2">
  <div class="panel">
    <div class="plabel">08 // Variants</div>
    <div class="ptitle">ATTENTION TYPES</div>
    <div class="bar or"></div>
    <div class="org-row"><span class="org-row-label">Self-attention</span><span class="org-row-val" style="color:var(--cyan);">Q,K,V all from X</span></div>
    <div class="org-row"><span class="org-row-label">Cross-attention</span><span class="org-row-val">Q from decoder, K/V from encoder</span></div>
    <div class="org-row"><span class="org-row-label">Causal / masked</span><span class="org-row-val" style="color:var(--yellow);">mask future tokens (GPT)</span></div>
    <div class="org-row"><span class="org-row-label">Grouped-query (GQA)</span><span class="org-row-val">fewer K/V heads, share across Q heads</span></div>
    <div class="org-row"><span class="org-row-label">Multi-query (MQA)</span><span class="org-row-val">one K/V head, many Q heads</span></div>
    <div class="org-row"><span class="org-row-label">Flash Attention</span><span class="org-row-val" style="color:var(--green);">IO-aware exact attention, no approx</span></div>
    <div class="org-row"><span class="org-row-label">Linear attention</span><span class="org-row-val">O(n) approximation via kernel trick</span></div>
    <p style="margin-top:10px;font-size:10px;color:#555;">GQA is now standard in production LLMs (Llama, Mistral, Gemma). Reduces KV cache memory by 4-8x with minimal quality loss. Flash Attention is the standard implementation — tiles computation to fit SRAM, eliminating the bottleneck of reading/writing the full n×n matrix to HBM.</p>
    <div style="margin-top:6px;"><span class="tag cy">GPT = causal</span><span class="tag">BERT = bidirectional</span><span class="tag cy">Llama = GQA</span></div>
  </div>

  <div class="panel">
    <div class="plabel">09 // Complexity</div>
    <div class="ptitle rd">THE O(n²) PROBLEM</div>
    <div class="bar rd"></div>
    <div class="math-block rd"><span class="eq rd">Time:   O(n² · d)</span><br><span class="eq rd">Memory: O(n²)</span><br><span style="color:#444;font-size:10px;">n = sequence length, d = model dim</span></div>
    <p style="font-size:10px;color:#555;">For n=4096, the attention matrix alone requires 4096² = 16.7M entries. At fp16 that's 33MB per head per layer. A 32-layer, 32-head model: 33MB × 32 × 32 = 33GB just for attention matrices. This is why long-context models are expensive.</p>
    <p style="font-size:10px;color:#555;">Flash Attention sidesteps materializing the full n×n matrix by fusing the softmax + matmul into tiled operations that stay in SRAM. Memory drops to O(n). This is how 128K+ context windows became practical.</p>
    <div style="display:grid;grid-template-columns:1fr 1fr;gap:4px;margin-top:10px;">
      <div style="background:#060606;border:1px solid var(--border);padding:10px;text-align:center;">
        <div style="font-family:'Bebas Neue',sans-serif;font-size:20px;color:var(--red);">Standard</div>
        <div style="font-family:'Bebas Neue',sans-serif;font-size:28px;color:var(--red);">O(n²)</div>
        <div style="font-size:9px;color:#333;">memory</div>
      </div>
      <div style="background:#060606;border:1px solid var(--border);padding:10px;text-align:center;">
        <div style="font-family:'Bebas Neue',sans-serif;font-size:20px;color:var(--green);">Flash Attn</div>
        <div style="font-family:'Bebas Neue',sans-serif;font-size:28px;color:var(--green);">O(n)</div>
        <div style="font-size:9px;color:#333;">memory</div>
      </div>
    </div>
    <div style="margin-top:8px;"><span class="tag rd">bottleneck</span><span class="tag gr">Flash Attn</span><span class="tag">Dao 2022</span></div>
  </div>
</div>

<!-- 07 POSITIONAL ENCODING -->
<div class="g2">
  <div class="panel">
    <div class="plabel">10 // Positional Encoding</div>
    <div class="ptitle">POSITION IS NOT FREE</div>
    <div class="bar yl"></div>
    <p>Attention is <span class="hl yl">permutation-invariant</span> — shuffle the tokens and the same values flow through (just re-routed). There's no inherent notion of position. Position must be injected explicitly.</p>
    <div class="math-block yl"><span class="eq yl">PE(pos, 2i)   = sin(pos / 10000^(2i/d))</span><br><span class="eq yl">PE(pos, 2i+1) = cos(pos / 10000^(2i/d))</span><br><span style="color:#444;font-size:10px;">Original sinusoidal encoding — Vaswani 2017</span></div>
    <p style="font-size:10px;color:#555;">Added to the input embeddings before the first layer. Learned absolute position embeddings (BERT) are simpler but don't generalize to lengths unseen during training. RoPE (Rotary Position Embedding) — used in GPT-NeoX, Llama — encodes relative position directly in the QK dot product via rotation matrices. Better extrapolation, now standard.</p>
    <div style="margin-top:6px;"><span class="tag yl">sinusoidal</span><span class="tag yl">RoPE</span><span class="tag">ALiBi</span><span class="tag">learned</span></div>
  </div>

  <div class="panel">
    <div class="plabel">11 // Interpretability</div>
    <div class="ptitle">WHAT HEADS LEARN</div>
    <div class="bar pu"></div>
    <p>Attention heads develop interpretable specializations through training. Elhage et al. (2021) and Anthropic mechanistic interpretability work identified recurring head types:</p>
    <div class="org-row"><span class="org-row-label">Induction heads</span><span class="org-row-val" style="color:var(--cyan);">copy patterns [A][B]...[A]→[B]</span></div>
    <div class="org-row"><span class="org-row-label">Previous token</span><span class="org-row-val">attend to pos i-1</span></div>
    <div class="org-row"><span class="org-row-label">Name lookup</span><span class="org-row-val" style="color:var(--yellow);">retrieve attributes of named entity</span></div>
    <div class="org-row"><span class="org-row-label">Syntactic</span><span class="org-row-val">subject-verb-object relations</span></div>
    <div class="org-row"><span class="org-row-label">Coreference</span><span class="org-row-val" style="color:var(--magenta);">pronoun → referent linking</span></div>
    <p style="margin-top:10px;font-size:10px;color:#555;">Induction heads in particular are thought to underlie in-context learning — the model's ability to follow patterns demonstrated in the prompt. They form during a phase transition in training and are a candidate mechanism for many few-shot capabilities.</p>
    <div style="margin-top:6px;"><span class="tag pu">Elhage 2021</span><span class="tag pu">induction heads</span><span class="tag">ICL mechanism</span></div>
  </div>
</div>

<div class="footer">
  <span>VASWANI ET AL. 2017 // ELHAGE ET AL. 2021 // DAO ET AL. 2022 // RAMSAUER ET AL. 2020</span>
  <span>BRUTALIST TERMINAL v2 // ATTENTION MECHANISM</span>
</div>

<script>
// ================================================================
// Attention heatmap demo
// ================================================================
const sentence = ['The','animal','did','not','cross','the','street','because','it','was','tired'];

const attnWeights = {
  'The':    [0.45,0.20,0.05,0.02,0.05,0.10,0.05,0.02,0.03,0.02,0.01],
  'animal': [0.15,0.35,0.04,0.03,0.08,0.06,0.04,0.05,0.12,0.05,0.03],
  'did':    [0.06,0.08,0.30,0.15,0.12,0.05,0.06,0.07,0.04,0.04,0.03],
  'not':    [0.04,0.05,0.22,0.28,0.16,0.04,0.05,0.07,0.04,0.03,0.02],
  'cross':  [0.05,0.10,0.08,0.08,0.28,0.06,0.10,0.09,0.06,0.04,0.06],
  'the':    [0.25,0.08,0.03,0.02,0.05,0.30,0.16,0.04,0.03,0.02,0.02],
  'street': [0.04,0.06,0.05,0.04,0.18,0.12,0.34,0.08,0.03,0.03,0.03],
  'because':[0.04,0.08,0.06,0.08,0.10,0.04,0.08,0.28,0.12,0.06,0.06],
  'it':     [0.04,0.32,0.03,0.03,0.05,0.03,0.10,0.10,0.18,0.06,0.06],
  'was':    [0.03,0.10,0.05,0.04,0.05,0.03,0.05,0.10,0.24,0.22,0.09],
  'tired':  [0.03,0.18,0.04,0.03,0.05,0.03,0.06,0.08,0.20,0.18,0.12],
};

// normalize rows
Object.keys(attnWeights).forEach(k => {
  const row = attnWeights[k];
  const sum = row.reduce((a,b)=>a+b,0);
  attnWeights[k] = row.map(v=>v/sum);
});

const tokenRow = document.getElementById('tokenRow');
const attnHeatmap = document.getElementById('attnHeatmap');

// build token buttons
sentence.forEach((word, i) => {
  const el = document.createElement('div');
  el.style.cssText = 'flex:1;padding:10px 4px;text-align:center;font-size:10px;font-weight:700;background:#0d0d0d;cursor:pointer;transition:all 0.1s;border:2px solid transparent;color:#cccccc;letter-spacing:1px;user-select:none;';
  el.textContent = word;
  el.addEventListener('mouseenter', () => { if (!el.classList.contains('sel')) el.style.background = '#1e1e1e'; });
  el.addEventListener('mouseleave', () => { if (!el.classList.contains('sel')) el.style.background = '#0d0d0d'; });
  el.addEventListener('click', () => selectToken(i, el, word));
  tokenRow.appendChild(el);
});

// build heatmap cells
sentence.forEach((word, i) => {
  const cell = document.createElement('div');
  cell.id = `ac-${i}`;
  cell.style.cssText = 'flex:1;display:flex;flex-direction:column;align-items:center;justify-content:center;gap:3px;padding:8px 2px;background:#111;transition:background 0.3s;min-height:70px;';
  cell.innerHTML = `
    <div id="av-${i}" style="font-family:'Bebas Neue',sans-serif;font-size:16px;letter-spacing:1px;color:#3a3a3a;transition:color 0.3s;">—</div>
    <div style="width:80%;height:3px;background:#1e1e1e;"><div id="ab-${i}" style="height:100%;width:0%;background:var(--red);transition:width 0.4s;box-shadow:0 0 4px var(--red);"></div></div>
    <div style="font-size:8px;color:#282828;letter-spacing:1px;">${word}</div>
  `;
  attnHeatmap.appendChild(cell);
});

function selectToken(idx, el, word) {
  // reset all
  document.querySelectorAll('#tokenRow > div').forEach(t => {
    t.style.borderColor = 'transparent';
    t.style.background = '#0d0d0d';
    t.style.color = '#cccccc';
    t.classList.remove('sel');
  });
  el.style.borderColor = 'var(--red)';
  el.style.background = 'rgba(255,45,85,0.08)';
  el.style.color = '#ffffff';
  el.classList.add('sel');

  const weights = attnWeights[word];
  document.getElementById('selectedTokenLabel').textContent = `"${word}" attending to:`;

  weights.forEach((w, i) => {
    const valEl = document.getElementById(`av-${i}`);
    const barEl = document.getElementById(`ab-${i}`);
    const cellEl = document.getElementById(`ac-${i}`);
    valEl.textContent = (w*100).toFixed(1)+'%';
    const alpha = Math.min(0.1 + w*3, 1);
    valEl.style.color = `rgba(255,45,85,${alpha})`;
    barEl.style.width = Math.min(w*100*4, 100)+'%';
    cellEl.style.background = `rgba(255,45,85,${Math.min(w*1.8,0.28)})`;
  });

  const maxW = Math.max(...weights);
  const maxIdx = weights.indexOf(maxW);
  const infos = {
    'it': `"it" attends most strongly to "animal" (${(weights[1]*100).toFixed(1)}%) — the model has resolved the pronoun to its correct referent rather than "street."`,
    'tired': `"tired" pulls attention back to "animal" — the adjective semantically anchors to the subject of the clause.`,
    'not': `Negation token strongly attends to "did" and "cross" — marking which verb and action it modifies.`,
    'because': `Causal connector attends broadly to both the main clause and the subordinate clause it introduces.`,
    'street': `"street" is not the referent of "it" despite proximity — attention correctly distributes away from it.`,
  };
  const info = infos[word] || `"${word}" attends most strongly to "${sentence[maxIdx]}" (${(maxW*100).toFixed(1)}%). Each head independently learns which relationships matter for prediction.`;
  document.getElementById('selectedInfo').textContent = info;
}

// ================================================================
// Multi-head canvas
// ================================================================
const mhc = document.getElementById('mhCanvas');
const mhctx = mhc.getContext('2d');

const HEAD_COLORS = ['#ff2d55','#00ffe9','#39ff14','#ffaa00','#e900ff','#ff6b00','#00bcd4','#ffe900'];
const HEAD_LABELS = ['syntax','coref','semantic','position','negation','copy','entity','temporal'];

// build legend
const legend = document.getElementById('headLegend');
HEAD_COLORS.forEach((col,i) => {
  const el = document.createElement('div');
  el.style.cssText = `display:inline-flex;align-items:center;gap:5px;font-size:9px;letter-spacing:1px;text-transform:uppercase;color:#333;margin-right:6px;`;
  el.innerHTML = `<div style="width:8px;height:8px;background:${col};"></div>${HEAD_LABELS[i]}`;
  legend.appendChild(el);
});

function drawMH() {
  const dpr = window.devicePixelRatio || 1;
  const rect = mhc.getBoundingClientRect();
  mhc.width = rect.width * dpr;
  mhc.height = 300 * dpr;
  mhctx.scale(dpr, dpr);
  const W = rect.width, H = 300;

  mhctx.fillStyle = '#060606'; mhctx.fillRect(0,0,W,H);

  // grid lines
  mhctx.strokeStyle = '#111'; mhctx.lineWidth = 1;
  for (let x = 0; x < W; x += 40) { mhctx.beginPath(); mhctx.moveTo(x,0); mhctx.lineTo(x,H); mhctx.stroke(); }
  for (let y = 0; y < H; y += 40) { mhctx.beginPath(); mhctx.moveTo(0,y); mhctx.lineTo(W,y); mhctx.stroke(); }

  const tokens = ['The','cat','sat','on','the','mat'];
  const nT = tokens.length;
  const numHeads = 8;
  const offX = 116;
  const offY = 22;
  const tokenW = (W - offX - 10) / nT;
  const headH = (H - offY - 8) / numHeads;

  // token headers
  mhctx.fillStyle = '#3a3a3a';
  mhctx.font = '9px Space Mono';
  mhctx.textAlign = 'center';
  tokens.forEach((t, i) => mhctx.fillText(t, offX + i*tokenW + tokenW/2, 14));

  // each head
  for (let h = 0; h < numHeads; h++) {
    const y = offY + h * headH;
    const col = HEAD_COLORS[h];
    const rr = parseInt(col.slice(1,3),16);
    const gg = parseInt(col.slice(3,5),16);
    const bb = parseInt(col.slice(5,7),16);

    // head label
    mhctx.fillStyle = col;
    mhctx.font = 'bold 9px Space Mono';
    mhctx.textAlign = 'right';
    mhctx.fillText(`H${h+1}`, 50, y + headH/2 + 3);
    mhctx.fillStyle = '#282828';
    mhctx.font = '8px Space Mono';
    mhctx.fillText(HEAD_LABELS[h], 110, y + headH/2 + 3);

    // separator
    mhctx.strokeStyle = '#111'; mhctx.lineWidth = 1;
    mhctx.beginPath(); mhctx.moveTo(0, y + headH); mhctx.lineTo(W, y + headH); mhctx.stroke();

    // attention circles
    for (let ki = 0; ki < nT; ki++) {
      let w = 0.05;
      const tk = tokens[ki], tq = tokens[Math.floor(nT/2)];
      if (HEAD_LABELS[h] === 'syntax')   w = Math.exp(-Math.abs(2-ki)*1.1)*0.5 + 0.02;
      else if (HEAD_LABELS[h] === 'coref')    w = (tk==='the'||tk==='The') ? 0.55 : 0.05;
      else if (HEAD_LABELS[h] === 'semantic') { const s=['cat','sat','mat']; w = s.includes(tk)?0.42:0.04; }
      else if (HEAD_LABELS[h] === 'position') w = ki===2 ? 0.6 : (ki===1?0.25:0.04);
      else if (HEAD_LABELS[h] === 'negation') w = ki===0 ? 0.5 : 0.07;
      else if (HEAD_LABELS[h] === 'copy')     w = ki===2 ? 0.8 : 0.04;
      else if (HEAD_LABELS[h] === 'entity')   w = (tk==='cat'||tk==='mat') ? 0.4 : 0.05;
      else                                    w = ki<=2 ? 0.18+ki*0.06 : 0.05;

      const alpha = Math.min(w * 0.9, 0.85);
      const r = Math.max(2, Math.min(headH*0.44*w*2, headH*0.44));
      mhctx.fillStyle = `rgba(${rr},${gg},${bb},${alpha})`;
      mhctx.shadowColor = col; mhctx.shadowBlur = w > 0.3 ? 10 : 0;
      mhctx.beginPath();
      mhctx.arc(offX + ki*tokenW + tokenW/2, y + headH/2, r, 0, Math.PI*2);
      mhctx.fill();
      mhctx.shadowBlur = 0;
    }
  }

  // column dividers
  mhctx.strokeStyle = '#1a1a1a'; mhctx.lineWidth = 1;
  for (let i = 0; i <= nT; i++) {
    const x = offX + i*tokenW;
    mhctx.beginPath(); mhctx.moveTo(x, offY); mhctx.lineTo(x, H-4); mhctx.stroke();
  }

  mhctx.fillStyle = '#282828';
  mhctx.font = '8px Space Mono';
  mhctx.textAlign = 'left';
  mhctx.fillText('circle radius encodes attention weight from each head to each token position', offX, H-6);
}

window.addEventListener('load', drawMH);
window.addEventListener('resize', drawMH);
</script>
</body>
</html>
